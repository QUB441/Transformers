{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Magic(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Magic, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Multi-Head Attention components\n",
    "        self.Q_linear = nn.Linear(d_model, d_model)\n",
    "        self.K_linear = nn.Linear(d_model, d_model)\n",
    "        self.V_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Feedforward Network (FFN)\n",
    "        self.ffn1 = nn.Linear(d_model, d_ff)\n",
    "        self.ffn2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.Q_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.K_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.V_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores: Q @ K.T\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) / self.d_k ** 0.5  # Scale attention scores\n",
    "        \n",
    "        mask = torch.triu(torch.ones_like(attn), diagonal=1).bool()\n",
    "        attn = attn.masked_fill(mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Attention output\n",
    "        output = torch.matmul(attn, V).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_linear(output)\n",
    "\n",
    "        # Add & Normalize\n",
    "        x = self.norm1(x + output)\n",
    "\n",
    "        # Feed-Forward Network (FFN)\n",
    "        ffn_out = F.relu(self.ffn1(x))\n",
    "        ffn_out = self.ffn2(ffn_out)\n",
    "\n",
    "        # Add & Normalize\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class multiBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=80, n_heads=2, d_ff=2048, max_length=512):\n",
    "        super(multiBERT, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.create_positional_encoding(max_length, d_model)\n",
    "        self.magics = nn.ModuleList([Magic(d_model, n_heads, d_ff) for _ in range(3)])\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def create_positional_encoding(self, max_length, embed_dim):\n",
    "        position = torch.arange(max_length, dtype=torch.float).unsqueeze(1)\n",
    "        dim = torch.arange(embed_dim, dtype=torch.float).unsqueeze(0)\n",
    "        angles = position / (10000 ** (dim / embed_dim))\n",
    "        pos_encoding = torch.zeros(max_length, embed_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self.emb(inputs)\n",
    "\n",
    "        # Add positional encoding (broadcast to match batch size)\n",
    "        embs += self.positional_encoding[:embs.size(1), :].to(embs.device)\n",
    "\n",
    "        # Pass through Magic layers\n",
    "        for magic in self.magics:\n",
    "            embs = magic(embs)\n",
    "\n",
    "        return embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0, 'A': 1, 'B': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {char: idx for idx, char in enumerate(set(data))}  # Create a vocab dictionary\n",
    "encoded_data = [vocab[char] for char in data]  # Convert data to indices\n",
    "vocab_size = len(vocab)  # Total number of unique tokens\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 2], [1, 2, 2], [2, 2, 0], [2, 0, 0], [0, 0, 1], [0, 1, 1], [1, 1, 2], [1, 2, 2], [2, 2, 0], [2, 0, 0]]\n",
      "[2, 0, 0, 1, 1, 2, 2, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 3  # Length of each input sequence\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(len(encoded_data) - sequence_length):\n",
    "    inputs.append(encoded_data[i:i+sequence_length])\n",
    "    outputs.append(encoded_data[i+sequence_length])\n",
    "\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#convert to tensors\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)  # Input sequences\n",
    "outputs = torch.tensor(outputs, dtype=torch.long)  # Target next characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 80  # Dimension of embeddings, can be adjusted\n",
    "n_heads = 2  # Number of attention heads\n",
    "d_ff = 160  # Dimension of feed-forward network\n",
    "\n",
    "model = multiBERT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, d_ff=d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliquid-candidate\u001b[0m (\u001b[33mliquid-candidate-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\py_crack\\MLX_wk5\\wandb\\run-20241111_152747-yxxyvy6b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liquid-candidate-personal/transformers/runs/yxxyvy6b' target=\"_blank\">transforming bert</a></strong> to <a href='https://wandb.ai/liquid-candidate-personal/transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liquid-candidate-personal/transformers' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liquid-candidate-personal/transformers/runs/yxxyvy6b' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/transformers/runs/yxxyvy6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.1093\n",
      "Epoch 10, Loss: 0.3003\n",
      "Epoch 20, Loss: 0.0607\n",
      "Epoch 30, Loss: 0.0261\n",
      "Epoch 40, Loss: 0.0163\n",
      "Epoch 50, Loss: 0.0126\n",
      "Epoch 60, Loss: 0.0108\n",
      "Epoch 70, Loss: 0.0098\n",
      "Epoch 80, Loss: 0.0091\n",
      "Epoch 90, Loss: 0.0086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>█▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>0.00823</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transforming bert</strong> at: <a href='https://wandb.ai/liquid-candidate-personal/transformers/runs/yxxyvy6b' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/transformers/runs/yxxyvy6b</a><br/> View project at: <a href='https://wandb.ai/liquid-candidate-personal/transformers' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241111_152747-yxxyvy6b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"transformers\", name='transforming bert')\n",
    "epochs = 100  # Number of training epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "    predictions = predictions[:, -1, :]  # Only take the last output for each input sequence\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(predictions, outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log the loss value\n",
    "    wandb.log({'average_loss': loss.item()})\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next character: A\n"
     ]
    }
   ],
   "source": [
    "def predict_next(model, input_seq, vocab):\n",
    "    model.eval()\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        last_output = output[:, -1, :]  # Get the last time step output\n",
    "        predicted_idx = torch.argmax(last_output, dim=-1).item()\n",
    "        for char, idx in vocab.items():\n",
    "            if idx == predicted_idx:\n",
    "                return char\n",
    "\n",
    "# # Example usage\n",
    "# input_seq = [vocab['C'], vocab['C'], vocab['B']]  # Sequence to predict next character\n",
    "# predicted_char = predict_next(model, input_seq, vocab)\n",
    "# print(f\"Predicted next character: {predicted_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sequence: ['B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "# Predict the next 20 characters\n",
    "predicted = []\n",
    "start_seq = ['A', 'A', 'B']\n",
    "\n",
    "while len(predicted) < 20:\n",
    "    input_seq_tokens = [vocab.get(token) for token in start_seq]\n",
    "    predicted_char = predict_next(model, input_seq_tokens, vocab)\n",
    "    predicted.append(predicted_char)\n",
    "    start_seq.append(predicted_char)  # Append character itself, not its vocab ID\n",
    "    start_seq = start_seq[1:]  # Keep the length of start_seq fixed\n",
    "\n",
    "print(\"Predicted sequence:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100  # Number of training epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "    predictions = predictions[:, -1, :]  # Only take the last output for each input sequence\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(predictions, outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Read the file\n",
    "with open('sentence_test_data.txt', \"r\") as f:\n",
    "    sentences = f.read().splitlines()  # Each line is a separate sentence\n",
    "\n",
    "def create_lookup_tables(sentences: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    # Split sentences into individual words\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(sentence.split())  # Split each sentence and add words to the list\n",
    "\n",
    "    # Count unique words and sort by frequency\n",
    "    word_counts = collections.Counter(words)\n",
    "    vocab = sorted(word_counts, key=lambda k: word_counts[k], reverse=True)\n",
    "    \n",
    "    # Create int-to-word and word-to-int mappings with special tokens\n",
    "    int_to_vocab = {ii + 1: word for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab[0] = '<PAD>'\n",
    "    int_to_vocab[len(int_to_vocab)] = '<UNK>'  # Add '<UNK>' token at the end\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "# Creating dictionary\n",
    "words_to_ids, ids_to_words = create_lookup_tables(sentences)\n",
    "\n",
    "# Tokenize sentences\n",
    "tokens = []\n",
    "for sentence in sentences:\n",
    "    # Split the sentence into words and convert to IDs\n",
    "    sentence_tokens = [words_to_ids.get(word, words_to_ids['<UNK>']) for word in sentence.split()]\n",
    "    tokens.append(sentence_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
