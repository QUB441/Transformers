{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoteGenerator:\n",
    "    def __init__(self, notes, length=8):\n",
    "        self.notes = notes  # List of possible notes, like ['A', 'B', 'C', ...]\n",
    "        self.store = []     # Stores generated sequences\n",
    "        self.genre = \"rock\" # Default genre\n",
    "        self.length = length  # Desired length of generated sequence\n",
    "        \n",
    "        # Define genre patterns\n",
    "        self.patterns = {\n",
    "            \"rock\": [\"C\", \"C\", \"G\", \"G\", \"A\", \"A\", \"G\"],\n",
    "            \"pop\": [\"C\", \"G\", \"A\", \"F\"],\n",
    "            \"blues\": [\"E\", \"E\", \"G\", \"G\", \"A\", \"Bb\", \"B\"]\n",
    "        }\n",
    "        \n",
    "    def set_genre(self, genre):\n",
    "        # Set the genre and validate if it exists in patterns\n",
    "        if genre.lower() in self.patterns:\n",
    "            self.genre = genre.lower()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported genre. Available genres: rock, pop, blues\")\n",
    "    \n",
    "    def generate(self):\n",
    "        # Fetch pattern based on the genre\n",
    "        base_pattern = self.patterns.get(self.genre, [])\n",
    "        \n",
    "        if not base_pattern:\n",
    "            raise ValueError(f\"No pattern defined for genre '{self.genre}'\")\n",
    "        \n",
    "        # Repeat the base pattern to reach the specified length\n",
    "        generated_sequence = []\n",
    "        for _ in range(self.length // len(base_pattern) + 1):\n",
    "            generated_sequence.extend(base_pattern)\n",
    "        \n",
    "        # Trim the generated sequence to the specified length\n",
    "        generated_sequence = generated_sequence[:self.length]\n",
    "        \n",
    "        # Store the generated sequence in `store`\n",
    "        self.store.append(generated_sequence)\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "    def add_genre(self, genre_name, pattern):\n",
    "        \"\"\"\n",
    "        Adds a new genre with a specified note pattern.\n",
    "        \"\"\"\n",
    "        self.patterns[genre_name.lower()] = pattern\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "notes = ['C', 'D', 'E', 'F', 'G', 'A', 'B']  # Just a list of possible notes for initialization\n",
    "note_generator = NoteGenerator(notes, length=1000)\n",
    "\n",
    "# Set genre to rock and generate a sequence\n",
    "note_generator.set_genre(\"rock\")\n",
    "#print(\"Rock sequence:\", note_generator.generate())\n",
    "\n",
    "rock_notes = ['<rock>'] + note_generator.generate()\n",
    "\n",
    "# Set genre to pop and generate a sequence\n",
    "note_generator.set_genre(\"pop\")\n",
    "#print(\"Pop sequence:\", note_generator.generate())\n",
    "pop_notes = ['<pop>'] + note_generator.generate()\n",
    "\n",
    "# # Adding and generating a sequence for a custom genre\n",
    "# note_generator.add_genre(\"custom\", [\"D\", \"D\", \"G\", \"A\", \"C\"])\n",
    "# note_generator.set_genre(\"custom\")\n",
    "# print(\"Custom sequence:\", note_generator.generate())\n",
    "\n",
    "# Set genre to pop and generate a sequence\n",
    "note_generator.set_genre(\"blues\")\n",
    "#print(\"Pop sequence:\", note_generator.generate())\n",
    "blues_notes = ['<blues>'] + note_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "print(len(pop_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = rock_notes + pop_notes + blues_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of notes\n",
    "\n",
    "\n",
    "# Open a text file in write mode\n",
    "with open('all_notes.txt', mode='w') as file:\n",
    "    # Write each note on a new line\n",
    "    for note in all_notes:\n",
    "        file.write(f\"{note}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<rock>', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', '<pop>', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', '<blues>', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb']\n"
     ]
    }
   ],
   "source": [
    "print(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is torch.Size([3, 6])\n",
      "embs shape: torch.Size([3, 6, 40])\n",
      "pos_enc shape: torch.Size([3, 6, 40])\n",
      "embs shape after adding pos_enc: torch.Size([3, 6, 40])\n",
      "torch.Size([3, 6, 12])\n",
      "tensor([[[-8.1787e-01, -1.2786e-01, -2.6768e-01,  1.5458e-02,  6.9480e-02,\n",
      "          -8.9042e-01,  2.4884e-01, -3.2139e-01, -5.8098e-01,  2.1978e-01,\n",
      "          -1.1136e-01,  6.9214e-01],\n",
      "         [-1.2042e+00, -4.0555e-01, -5.0192e-02,  2.4976e-01, -1.1256e+00,\n",
      "          -4.4741e-02,  3.4121e-01, -3.8747e-01, -7.6250e-02, -4.4969e-01,\n",
      "           3.4417e-01, -4.4099e-02],\n",
      "         [-3.8031e-01, -7.3198e-02, -1.1705e-01,  1.4237e-01, -2.0626e-01,\n",
      "           1.7949e-01, -1.9510e-01, -1.5474e-01, -8.0424e-01,  2.9966e-01,\n",
      "          -2.0770e-01,  9.0650e-01],\n",
      "         [-9.8126e-01, -1.3597e-01, -1.7067e-01,  6.4180e-01,  2.0481e-01,\n",
      "          -4.2721e-01,  3.7970e-01, -1.0480e+00, -6.9986e-01,  7.6501e-01,\n",
      "           5.6108e-04,  5.5375e-01],\n",
      "         [-7.5134e-01, -2.3798e-01,  8.3690e-01,  1.6542e+00, -5.1273e-01,\n",
      "          -5.5489e-01,  1.2691e+00,  1.7829e-01,  2.1345e-01, -1.9447e-01,\n",
      "          -7.3320e-01,  1.4192e-01],\n",
      "         [-7.6213e-01, -1.8161e-01,  9.9636e-01,  1.7274e+00, -4.7900e-01,\n",
      "          -7.9093e-01,  1.1754e+00,  2.3737e-01,  3.7692e-01, -1.7130e-01,\n",
      "          -7.6721e-01,  1.8616e-01]],\n",
      "\n",
      "        [[-8.1787e-01, -1.2786e-01, -2.6768e-01,  1.5458e-02,  6.9480e-02,\n",
      "          -8.9042e-01,  2.4884e-01, -3.2139e-01, -5.8098e-01,  2.1978e-01,\n",
      "          -1.1136e-01,  6.9214e-01],\n",
      "         [-3.6608e-01, -2.3801e-01, -8.4241e-02,  1.1434e+00,  1.8504e-01,\n",
      "           3.7109e-01,  1.6837e-03, -3.1023e-01, -1.4163e+00, -1.7710e-01,\n",
      "          -4.0892e-01,  8.9474e-01],\n",
      "         [-3.2042e-01, -2.7993e-02, -2.4069e-01,  1.0535e-01, -7.7732e-02,\n",
      "           7.9693e-02, -2.4349e-01, -1.2853e-01, -9.9415e-01,  3.1322e-01,\n",
      "          -2.6612e-01,  1.0189e+00],\n",
      "         [-8.9587e-01, -1.0608e-01, -2.8990e-01,  6.3553e-01,  3.1052e-01,\n",
      "          -4.4976e-01,  3.5862e-01, -1.0295e+00, -9.0650e-01,  7.3057e-01,\n",
      "          -1.3340e-01,  7.1086e-01],\n",
      "         [-6.9804e-01, -2.1209e-01,  7.4879e-01,  1.6331e+00, -4.5793e-01,\n",
      "          -5.5943e-01,  1.2943e+00,  2.0786e-01,  4.2815e-02, -2.5467e-01,\n",
      "          -7.8811e-01,  2.7560e-01],\n",
      "         [ 1.9164e-01, -5.8535e-01,  1.3113e-01,  2.0054e-01,  4.7245e-01,\n",
      "          -1.2509e-01,  8.8429e-01,  1.1673e+00, -6.3366e-01, -8.0412e-01,\n",
      "          -1.0618e+00,  8.2490e-01]],\n",
      "\n",
      "        [[-8.1787e-01, -1.2786e-01, -2.6768e-01,  1.5458e-02,  6.9480e-02,\n",
      "          -8.9042e-01,  2.4884e-01, -3.2139e-01, -5.8098e-01,  2.1978e-01,\n",
      "          -1.1136e-01,  6.9214e-01],\n",
      "         [-9.8193e-01,  3.0713e-01, -2.6527e-01,  3.3820e-01, -3.6458e-01,\n",
      "           2.5645e-01, -6.2616e-02, -8.5175e-01, -1.0320e+00, -1.4697e-01,\n",
      "           1.3601e-01,  1.9066e-01],\n",
      "         [-9.9307e-01, -2.1306e-01, -1.1092e-01,  4.4605e-01, -1.1116e+00,\n",
      "           4.5627e-01, -4.6321e-03,  6.2741e-02, -3.3498e-01, -1.9457e-01,\n",
      "           7.0184e-02, -1.4100e-01],\n",
      "         [-7.2824e-01, -3.0742e-03, -1.9540e-01,  6.3036e-01,  3.3907e-01,\n",
      "          -4.5409e-01,  3.1010e-01, -1.0458e+00, -7.6422e-01,  7.5575e-01,\n",
      "           1.3923e-02,  7.6380e-01],\n",
      "         [-5.2401e-01, -8.7718e-02,  8.5199e-01,  1.6977e+00, -3.8344e-01,\n",
      "          -5.7125e-01,  1.2864e+00,  1.1710e-01,  1.7415e-01, -2.0922e-01,\n",
      "          -6.7859e-01,  2.7485e-01],\n",
      "         [-4.6102e-01,  5.4179e-01,  3.0150e-01,  7.0254e-01,  6.0549e-01,\n",
      "          -9.6877e-01, -2.6720e-01, -5.2865e-01, -5.4860e-01,  2.8932e-01,\n",
      "          -1.1431e+00,  8.9781e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#let create decoder only model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Magic(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Magic, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Multi-Head Attention components\n",
    "        self.Q_linear = nn.Linear(d_model, d_model)\n",
    "        self.K_linear = nn.Linear(d_model, d_model)\n",
    "        self.V_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Feedforward Network (FFN)\n",
    "        self.ffn1 = nn.Linear(d_model, d_ff)\n",
    "        self.ffn2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.Q_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.K_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.V_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores: Q @ K.T and apply scaling\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) / self.d_k ** 0.5  # Scale attention scores\n",
    "\n",
    "        # Create a mask to prevent attending to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        attn = attn.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Apply softmax to normalized scores\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Compute the attention output\n",
    "        output = torch.matmul(attn, V).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_linear(output)\n",
    "\n",
    "        # Add & Normalize\n",
    "        x = self.norm1(x + output)\n",
    "\n",
    "        # Feed-Forward Network (FFN)\n",
    "        ffn_out = F.relu(self.ffn1(x))\n",
    "        ffn_out = self.ffn2(ffn_out)\n",
    "\n",
    "        # Add & Normalize\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, d_model=80, n_heads=2, d_ff=2048, max_length=10000):\n",
    "        super(TransDecoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.max_length = max_length\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.create_positional_encoding(max_length, d_model)\n",
    "        self.magics = nn.ModuleList([Magic(d_model, n_heads, d_ff) for _ in range(3)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "\n",
    "    def create_positional_encoding(self, seq_len, embed_dim):\n",
    "        position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        dim = torch.arange(embed_dim, dtype=torch.float).unsqueeze(0)\n",
    "        angles = position / (10000 ** (dim / embed_dim))\n",
    "        pos_encoding = torch.zeros(seq_len, embed_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Adjust for potential singleton middle dimension in testing\n",
    "        if inputs.dim() == 3 and inputs.size(1) == 1:\n",
    "            inputs = inputs.squeeze(1)  # Squeeze the middle dimension if itâ€™s just [batch_size, 1, seq_len]\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embs = self.emb(inputs)  # Shape: [batch_size, seq_len, d_model]\n",
    "        print(f\"embs shape: {embs.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        pos_enc = self.positional_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1).to(embs.device)\n",
    "        print(f\"pos_enc shape: {pos_enc.shape}\")\n",
    "\n",
    "        embs = embs + pos_enc\n",
    "        print(f\"embs shape after adding pos_enc: {embs.shape}\")\n",
    "\n",
    "        for magic in self.magics:\n",
    "            embs = magic(embs)\n",
    "\n",
    "        logits = self.output_layer(embs)\n",
    "        return logits\n",
    "\n",
    "        # # Create positional encoding with the correct batch and sequence dimensions\n",
    "        # pos_enc = self.create_positional_encoding(seq_len, self.d_model).unsqueeze(0).expand(batch_size, -1, -1).to(embs.device)\n",
    "        # print(f\"pos_enc shape: {pos_enc.shape}\")\n",
    "\n",
    "        # # Add positional encoding to input embeddings\n",
    "        # embs = embs + pos_enc\n",
    "        # print(f\"embs shape after adding pos_enc: {embs.shape}\")\n",
    "\n",
    "        # for magic in self.magics:\n",
    "        #     embs = magic(embs)\n",
    "\n",
    "        # logits = self.output_layer(embs)\n",
    "        # return logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is torch.Size([3, 6])\n",
      "embs shape: torch.Size([3, 6, 40])\n",
      "pos_enc shape: torch.Size([3, 6, 40])\n",
      "embs shape after adding pos_enc: torch.Size([3, 6, 40])\n",
      "torch.Size([3, 6, 12])\n",
      "tensor([[[ 2.3131e-01, -4.8160e-01, -3.5713e-01, -3.9369e-01,  2.0665e-01,\n",
      "          -5.0323e-01,  8.4316e-01,  9.7482e-01, -7.8712e-01,  2.9079e-03,\n",
      "          -1.2800e+00,  2.7560e-01],\n",
      "         [ 4.7081e-01, -1.4443e+00,  6.3215e-01,  9.8232e-01,  2.9088e-01,\n",
      "           3.6610e-02,  4.2767e-01,  9.4300e-01, -4.1064e-01, -4.3242e-02,\n",
      "          -7.6113e-01,  3.1141e-01],\n",
      "         [ 4.8520e-01, -8.0273e-01,  6.2346e-01,  4.8529e-01,  2.4321e-02,\n",
      "          -2.8222e-01,  3.7729e-01,  5.5123e-01, -2.5428e-01, -4.5207e-01,\n",
      "          -9.7834e-01, -2.4100e-01],\n",
      "         [ 3.4002e-02,  8.7531e-02,  1.9515e-01,  9.1648e-02, -1.1729e+00,\n",
      "          -1.1199e-01, -4.2721e-01, -2.1157e-01, -6.0469e-01, -5.0898e-01,\n",
      "          -3.9545e-01,  4.7066e-01],\n",
      "         [ 3.2034e-01, -1.1637e+00,  3.5254e-01,  6.9355e-01, -2.7315e-01,\n",
      "          -6.6104e-01,  3.6218e-01,  5.4464e-01, -6.4840e-01, -7.2856e-01,\n",
      "          -5.5806e-01,  2.5572e-01],\n",
      "         [ 4.3330e-01, -1.2235e+00,  2.3394e-01,  6.3059e-01, -3.0620e-01,\n",
      "          -7.7596e-01,  4.0477e-01,  6.2404e-01, -5.5102e-01, -6.5676e-01,\n",
      "          -4.7743e-01,  2.6245e-01]],\n",
      "\n",
      "        [[ 2.3131e-01, -4.8160e-01, -3.5713e-01, -3.9369e-01,  2.0665e-01,\n",
      "          -5.0323e-01,  8.4316e-01,  9.7482e-01, -7.8712e-01,  2.9079e-03,\n",
      "          -1.2800e+00,  2.7560e-01],\n",
      "         [-1.3369e-01,  9.2440e-02, -4.1642e-01,  1.3941e-01,  6.8709e-01,\n",
      "          -6.0435e-02,  8.3530e-01,  1.1747e+00, -1.0114e+00,  5.1390e-01,\n",
      "           1.3462e-02,  1.6355e-01],\n",
      "         [ 4.8056e-01, -6.9349e-01,  6.5800e-01,  5.3167e-01,  1.4414e-01,\n",
      "          -4.9647e-02,  6.8108e-01,  3.7870e-01, -2.4905e-01, -3.9827e-01,\n",
      "          -9.0590e-01, -3.5038e-01],\n",
      "         [-1.9996e-02,  1.8737e-01,  2.0789e-01,  1.1706e-01, -1.1672e+00,\n",
      "           7.9795e-04, -2.5293e-01, -3.6480e-01, -6.6627e-01, -4.4129e-01,\n",
      "          -3.5974e-01,  4.4736e-01],\n",
      "         [ 2.9111e-01, -1.0734e+00,  3.0629e-01,  7.2001e-01, -1.3455e-01,\n",
      "          -5.2710e-01,  4.6939e-01,  5.3134e-01, -6.5759e-01, -6.5179e-01,\n",
      "          -5.6243e-01,  2.6796e-01],\n",
      "         [ 4.3333e-02, -6.0542e-01, -2.3095e-01,  1.0646e+00,  5.0627e-01,\n",
      "          -2.5415e-01, -1.0782e-01, -6.7192e-02, -7.1521e-01,  2.0861e-01,\n",
      "          -4.3492e-01,  2.9406e-01]],\n",
      "\n",
      "        [[ 2.3131e-01, -4.8160e-01, -3.5713e-01, -3.9369e-01,  2.0665e-01,\n",
      "          -5.0323e-01,  8.4316e-01,  9.7482e-01, -7.8712e-01,  2.9079e-03,\n",
      "          -1.2800e+00,  2.7560e-01],\n",
      "         [ 3.7974e-02, -4.6123e-01,  3.2707e-01, -2.1270e-01, -3.3784e-01,\n",
      "          -7.0700e-01, -1.7036e-02, -2.4951e-01, -9.2565e-02, -4.8980e-01,\n",
      "           6.3444e-02, -2.0092e-01],\n",
      "         [-1.1854e+00,  4.1371e-01, -5.4248e-01, -1.1206e-01, -2.6960e-01,\n",
      "          -4.6748e-01, -7.2029e-01,  4.9175e-01, -1.6939e-02, -4.4868e-01,\n",
      "          -7.8788e-01,  6.1465e-01],\n",
      "         [-1.1678e-01,  2.4152e-01,  1.6357e-01,  9.5292e-02, -1.2284e+00,\n",
      "          -2.1003e-01, -4.1913e-01, -4.4164e-01, -7.1847e-01, -5.5780e-01,\n",
      "          -3.7174e-01,  5.4280e-01],\n",
      "         [ 1.7986e-01, -1.0177e+00,  2.6524e-01,  7.3186e-01, -3.0886e-01,\n",
      "          -6.8392e-01,  2.8244e-01,  3.9386e-01, -7.4644e-01, -8.2242e-01,\n",
      "          -5.8792e-01,  3.4920e-01],\n",
      "         [ 1.1800e-01, -4.8724e-01, -7.7438e-01,  7.8009e-01, -2.3327e-01,\n",
      "          -1.0263e+00, -3.1833e-03,  2.1145e-02, -2.2832e-01, -6.8297e-02,\n",
      "          -5.9719e-01,  6.3710e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Example usage\n",
    "    vocab_size = 12\n",
    "    d_model = 40\n",
    "    n_heads = 2\n",
    "    d_ff = 80\n",
    "    max_length = 512\n",
    "    batch_size = 1\n",
    "\n",
    "    model = TransDecoder(vocab_size, batch_size, d_model, n_heads, d_ff, max_length)\n",
    "    inputs = torch.tensor([[6, 0, 8, 11, 3, 3], [6, 4, 8, 11, 3, 2], [6, 5, 10, 11, 3, 7]])\n",
    "    print(\"input shape is\", inputs.shape)\n",
    "    outputs = model(inputs)\n",
    "    print(outputs.shape)\n",
    "    print(outputs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs shape: torch.Size([1, 40, 80])\n",
      "pos_enc shape: torch.Size([1, 40, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 40, 80])\n",
      "outputs shape: torch.Size([1, 40, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Create a test case\n",
    "batch_size = 1\n",
    "seq_len = 40\n",
    "d_model = 80\n",
    "vocab_size = 1000\n",
    "batch_size = 1\n",
    "\n",
    "inputs = torch.randint(0, vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "\n",
    "model = TransDecoder(vocab_size, batch_size, d_model=d_model)\n",
    "\n",
    "# Run the test case\n",
    "outputs = model(inputs)\n",
    "print(f\"outputs shape: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<rock>', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', 'G', 'C', 'C', 'G', 'G', 'A', 'A', '<pop>', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', 'C', 'G', 'A', 'F', '<blues>', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb', 'B', 'E', 'E', 'G', 'G', 'A', 'Bb']\n"
     ]
    }
   ],
   "source": [
    "print(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'<rock>': 0, 'B': 1, 'F': 2, 'A': 3, '<pop>': 4, '<blues>': 5, '<s>': 6, 'Bb': 7, 'C': 8, '</s>': 9, 'E': 10, 'G': 11}\n",
      "3003\n",
      "1002\n",
      "[6, 0, 8]\n",
      "[4, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 8, 11, 3, 2, 9]\n"
     ]
    }
   ],
   "source": [
    "#let make training loop\n",
    "# Define the dataset\n",
    "data = all_notes\n",
    "\n",
    "# Create the vocab dictionary, including start and end tokens\n",
    "special_tokens = ['<s>', '</s>']\n",
    "vocab = {char: idx for idx, char in enumerate(set(data).union(special_tokens))}\n",
    "vocab_size = len(vocab)  # Total number of unique tokens\n",
    "\n",
    "# Function to get indices of data\n",
    "def get_indices(natural_data):\n",
    "    return [vocab[char] for char in natural_data]\n",
    "\n",
    "# Encode the data with start and end tokens\n",
    "rock_train_data = [vocab['<s>']] + get_indices(rock_notes)\n",
    "pop_train_data = [vocab['<s>']] + get_indices(pop_notes)\n",
    "blues_train_data = [vocab['<s>']] + get_indices(blues_notes)\n",
    "\n",
    "rock_target_data = get_indices(rock_notes) + [vocab['</s>']]\n",
    "pop_target_data = get_indices(pop_notes) + [vocab['</s>']]\n",
    "blues_target_data = get_indices(blues_notes) + [vocab['</s>']]\n",
    "\n",
    "print(vocab_size)\n",
    "print(vocab)\n",
    "print(len(data))\n",
    "print(len(rock_train_data))\n",
    "print(rock_train_data[:3])\n",
    "print(pop_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<rock>': 0, 'B': 1, 'F': 2, 'A': 3, '<pop>': 4, '<blues>': 5, '<s>': 6, 'Bb': 7, 'C': 8, '</s>': 9, 'E': 10, 'G': 11}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 6,  0,  8,  ..., 11,  3,  3]), tensor([ 6,  4,  8,  ..., 11,  3,  2]), tensor([ 6,  5, 10,  ..., 11,  3,  7])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#turn into tensors\n",
    "\n",
    "rock_input = torch.tensor(rock_train_data, dtype=torch.long)\n",
    "rock_target = torch.tensor(rock_target_data, dtype=torch.long)\n",
    "\n",
    "# For Pop genre\n",
    "pop_input = torch.tensor(pop_train_data, dtype=torch.long)\n",
    "pop_target = torch.tensor(pop_target_data, dtype=torch.long)\n",
    "\n",
    "# For Blues genre\n",
    "blues_input = torch.tensor(blues_train_data, dtype=torch.long)\n",
    "blues_target = torch.tensor(blues_target_data, dtype=torch.long)\n",
    "\n",
    "# Organize these as lists of inputs and targets per genre\n",
    "inputs = [rock_input, pop_input, blues_input]\n",
    "targets = [rock_target, pop_target, blues_target]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs[0]))\n",
    "print(len(targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 40  # Dimension of embeddings, can be adjusted\n",
    "n_heads = 2  # Number of attention heads\n",
    "d_ff = 80  # Dimension of feed-forward network\n",
    "batch_size = 1  # Number of sequences processed in parallel\n",
    "\n",
    "model = TransDecoder(vocab_size=vocab_size, batch_size=batch_size, d_model=d_model, n_heads=n_heads, d_ff=d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape the input and target tensors to include batch dimension\n",
    "# rock_input = rock_input.unsqueeze(0)  # Shape: [batch_size, seq_len]\n",
    "# rock_target = rock_target.unsqueeze(0)  # Shape: [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize W&B\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransforming_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m  \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Reshape the input and target tensors to include batch dimension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1270\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[1;32m-> 1270\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\analytics\\sentry.py:161\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1256\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m   1255\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:848\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    846\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1\n",
    "\n",
    "model = TransDecoder(vocab_size, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"transformers\", name=\"transforming_decoder\")\n",
    "\n",
    "epochs = 150  # Number of training epochs\n",
    "\n",
    "# Reshape the input and target tensors to include batch dimension\n",
    "rock_input = rock_input.unsqueeze(0)  # Shape: [batch_size, seq_len]\n",
    "rock_target = rock_target.unsqueeze(0)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "print(f\"Training data shapes - Input: {rock_input.shape}, Target: {rock_target.shape}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(rock_input)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(predictions.view(-1, predictions.size(-1)), rock_target.view(-1))\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss\n",
    "    wandb.log({'average_loss': avg_loss})\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize W&B\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransforming_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m  \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[0;32m     15\u001b[0m d_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Dimension of embeddings, can be adjusted\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1270\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[1;32m-> 1270\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\analytics\\sentry.py:161\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1256\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m   1255\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:848\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    846\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "# #decoder with unzipping\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"transformers\", name=\"transforming_decoder\")\n",
    "epochs = 150  # Number of training epochs\n",
    "\n",
    "d_model = 50  # Dimension of embeddings, can be adjusted\n",
    "n_heads = 2  # Number of attention heads\n",
    "d_ff = 80  # Dimension of feed-forward network\n",
    "batch_size = 1  # Number of sequences processed in parallel\n",
    "\n",
    "model2 = TransDecoder(vocab_size=vocab_size, batch_size=batch_size, d_model=d_model, n_heads=n_heads, d_ff=d_ff)\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for genre_inputs, genre_targets in zip(inputs, targets):\n",
    "        # Ensure genre inputs and targets are tensors\n",
    "        genre_inputs = torch.tensor(genre_inputs, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "        print(\"genere inputs shape is\", genre_inputs.shape, \"genre  len is\", len(genre_inputs))\n",
    "        genre_targets = torch.tensor(genre_targets, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "        print(genre_targets.shape)  \n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(genre_inputs)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Compute the loss\n",
    "        #loss = criterion(predictions.view(-1, predictions.size(-1)), genre_targets.view(-1))\n",
    "        loss = criterion(predictions.view(-1, vocab_size), genre_targets.view(-1))\n",
    "        #total_loss += loss.item()\n",
    "        avg_loss = total_loss / (epoch + 1)  # Average loss over epochs\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss value\n",
    "        avg_loss = total_loss / len(inputs)\n",
    "        wandb.log({'average_loss': avg_loss})\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# model2 = TransDecoder(vocab_size=vocab_size, batch_size=batch_size, d_model=d_model, n_heads=n_heads, d_ff=d_ff)\n",
    "# optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Initialize W&B\n",
    "# #wandb.init(project=\"transformers\", name=\"transforming_decoder\")\n",
    "# epochs = 100\n",
    "\n",
    "# d_model = 50\n",
    "# n_heads = 2\n",
    "# d_ff = 80\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model2.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for genre_inputs, genre_targets in zip(inputs, targets):\n",
    "#         genre_inputs = torch.tensor(genre_inputs, dtype=torch.long).unsqueeze(0)  # Shape: [1, seq_len]\n",
    "#         genre_targets = torch.tensor(genre_targets, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "#         # Forward pass\n",
    "#         predictions = model2(genre_inputs)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = criterion(predictions.view(-1, vocab_size), genre_targets.view(-1))  # Ensure correct shape\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     avg_loss = total_loss / len(inputs)  # Average over the genre batches\n",
    "#     wandb.log({'average_loss': avg_loss})\n",
    "\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # Step the learning rate scheduler\n",
    "#     scheduler.step()\n",
    "\n",
    "# wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 7, 80])\n",
      "pos_enc shape: torch.Size([1, 7, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 7, 80])\n",
      "embs shape: torch.Size([1, 8, 80])\n",
      "pos_enc shape: torch.Size([1, 8, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 8, 80])\n",
      "embs shape: torch.Size([1, 9, 80])\n",
      "pos_enc shape: torch.Size([1, 9, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 9, 80])\n",
      "embs shape: torch.Size([1, 10, 80])\n",
      "pos_enc shape: torch.Size([1, 10, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 10, 80])\n",
      "embs shape: torch.Size([1, 11, 80])\n",
      "pos_enc shape: torch.Size([1, 11, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 11, 80])\n",
      "embs shape: torch.Size([1, 12, 80])\n",
      "pos_enc shape: torch.Size([1, 12, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 12, 80])\n",
      "embs shape: torch.Size([1, 13, 80])\n",
      "pos_enc shape: torch.Size([1, 13, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 13, 80])\n",
      "embs shape: torch.Size([1, 14, 80])\n",
      "pos_enc shape: torch.Size([1, 14, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 14, 80])\n",
      "embs shape: torch.Size([1, 15, 80])\n",
      "pos_enc shape: torch.Size([1, 15, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 15, 80])\n",
      "Generated Sequence: [3, 11, 8, 8, 8, 11, 11, 11, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, input_seq, max_len=20):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    input_tensor = torch.tensor(input_seq).unsqueeze(0)  # Shape: [1, seq_len]\n",
    "    predictions = []  # Store predicted tokens\n",
    "\n",
    "    # Iterate to generate the sequence\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Take the last token's logits (for next token prediction)\n",
    "        last_token_logits = output[0, -1, :]  # Get logits for the last token in the sequence\n",
    "        \n",
    "        # Get the predicted token by sampling (you could use argmax or a sampling method)\n",
    "        next_token = torch.argmax(last_token_logits).item()  # Take the token with the highest logit\n",
    "        \n",
    "        predictions.append(next_token)  # Append to the list of predictions\n",
    "        \n",
    "        # Add the predicted token to the input sequence for next prediction\n",
    "        input_tensor = torch.cat((input_tensor, torch.tensor([[next_token]]).to(input_tensor.device)), dim=1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_seq = [6, 0, 8, 11, 3, 3]  # Example input sequence\n",
    "generated_output = generate_sequence(model, input_seq, max_len=10)\n",
    "print(f\"Generated Sequence: {generated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(model, input_seq, vocab):\n",
    "    model.eval()\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        last_output = output[:, -1, :]  # Get the last time step output\n",
    "        predicted_idx = torch.argmax(last_output, dim=-1).item()\n",
    "        for char, idx in vocab.items():\n",
    "            if idx == predicted_idx:\n",
    "                return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "embs shape: torch.Size([1, 6, 80])\n",
      "pos_enc shape: torch.Size([1, 6, 80])\n",
      "embs shape after adding pos_enc: torch.Size([1, 6, 80])\n",
      "Predicted sequence: ['A', 'A', 'A', 'Bb', 'B', 'E', 'G', 'A', 'Bb', 'B']\n"
     ]
    }
   ],
   "source": [
    "# Predict the next 20 characters\n",
    "predicted = []\n",
    "start_seq = [\"<rock>\", \"C\", \"C\", \"G\", \"G\", \"A\"]\n",
    "\n",
    "while len(predicted) < 10:\n",
    "    input_seq_tokens = [vocab.get(token) for token in start_seq]\n",
    "    predicted_char = predict_next(model, input_seq_tokens, vocab)\n",
    "    predicted.append(predicted_char)\n",
    "    start_seq.append(predicted_char)  # Append character itself, not its vocab ID\n",
    "    start_seq = start_seq[1:]  # Keep the length of start_seq fixed\n",
    "\n",
    "print(\"Predicted sequence:\", predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #\"blues\": [\"E\", \"E\", \"G\", \"G\", \"A\", \"Bb\", \"B\"]\n",
    "    #  \"rock\": [\"C\", \"C\", \"G\", \"G\", \"A\", \"A\", \"G\"],\n",
    "    #         \"pop\": [\"C\", \"G\", \"A\", \"F\"],\n",
    "    #         \"blues\": [\"E\", \"E\", \"G\", \"G\", \"A\", \"Bb\", \"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<rock>', 1: 'B', 2: 'F', 3: 'A', 4: '<pop>', 5: '<blues>', 6: '<s>', 7: 'Bb', 8: 'C', 9: '</s>', 10: 'E', 11: 'G', 13: '<UNK>'}\n",
      "Warning: Out-of-range token detected in input aabb. Tokens: [6, 13, 13, 13, 13]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Assuming vocab_to_int and int_to_vocab are dictionaries you've already defined\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_string \u001b[38;5;129;01min\u001b[39;00m test_strings:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_to_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[192], line 36\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(model, input_sequence, vocab_to_int, int_to_vocab)\u001b[0m\n\u001b[0;32m     33\u001b[0m tokens_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokens)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Shape: [1, seq_len]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get model output (logits)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[0;32m     39\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[152], line 95\u001b[0m, in \u001b[0;36mTransDecoder.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Squeeze the middle dimension if itâ€™s just [batch_size, 1, seq_len]\u001b[39;00m\n\u001b[0;32m     94\u001b[0m batch_size, seq_len \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 95\u001b[0m embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, seq_len, d_model]\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m pos_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:seq_len, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(embs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add the '<UNK>' token with a unique index\n",
    "vocab['<UNK>'] = len(vocab)\n",
    "int_to_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(int_to_vocab)\n",
    "\n",
    "def infer(model, input_sequence, vocab_to_int, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Inference function to predict the next sequence based on the model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained model for inference.\n",
    "    - input_sequence: The input string to infer from.\n",
    "    - vocab_to_int: A dictionary mapping vocab words to integer IDs.\n",
    "    - int_to_vocab: A dictionary mapping integer IDs back to vocab words.\n",
    "    \n",
    "    Returns:\n",
    "    - A string with the predicted output sequence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    tokens = [vocab_to_int.get(char, vocab_to_int['<UNK>']) for char in input_sequence]  # Convert input characters to tokens\n",
    "    \n",
    "    # Add start token\n",
    "    tokens = [vocab_to_int['<s>']] + tokens  # Add start token at the beginning of the sequence\n",
    "    # Check if tokens have any out-of-range values\n",
    "    vocab_size = len(vocab_to_int)\n",
    "    if any(token >= vocab_size for token in tokens):\n",
    "        print(f\"Warning: Out-of-range token detected in input {input_sequence}. Tokens: {tokens}\")\n",
    "\n",
    "\n",
    "    # Convert tokens to tensor and reshape for batch processing\n",
    "    #tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(model.device)  # Shape: [1, seq_len]\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(next(model.parameters()).device)  # Shape: [1, seq_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens_tensor)  # Get model output (logits)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the predicted token indices (most probable)\n",
    "    predicted_tokens = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    # Convert the predicted token indices back to characters using int_to_vocab\n",
    "    predicted_tokens = predicted_tokens.squeeze(0).cpu().numpy()  # Squeeze the batch dimension and move to CPU\n",
    "\n",
    "    # Handle out-of-range predictions gracefully\n",
    "    output_tokens = [int_to_vocab.get(token, '<UNK>') for token in predicted_tokens]\n",
    "    \n",
    "    # Join the output tokens into a single string (excluding the start token)\n",
    "    return ''.join(output_tokens[1:])  # Skip the start token\n",
    "\n",
    "# Example test strings\n",
    "test_strings = ['aabb','a','aa']\n",
    "\n",
    "# Assuming vocab_to_int and int_to_vocab are dictionaries you've already defined\n",
    "for test_string in test_strings:\n",
    "    result = infer(model, test_string, vocab, int_to_vocab)\n",
    "    print(f\"Result for {test_string}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
